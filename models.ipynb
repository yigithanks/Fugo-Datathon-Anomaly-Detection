{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dd6b9-d1b9-4699-9838-34d73b96b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Assuming you have your data in a DataFrame 'train' with columns 'timestamp', 'value', and 'TARGET'\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X = train[['value']]\n",
    "y = train['TARGET']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "print(f\"P&R Score: {(precision+recall)/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e419459-7c97-4975-93aa-d186586eeb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "941161cf-9b77-4049-a59e-f900478cabb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  TARGET\n",
      "0      50000       0\n",
      "1      50001       0\n",
      "2      50002       0\n",
      "3      50003       0\n",
      "4      50004       0\n",
      "...      ...     ...\n",
      "49995  99995       0\n",
      "49996  99996       0\n",
      "49997  99997       0\n",
      "49998  99998       0\n",
      "49999  99999       0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your data in a DataFrame 'train' with columns 'timestamp', 'value', and 'TARGET'\n",
    "\n",
    "# Convert the 'timestamp' column to datetime type\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X_train = train['value']  # Include the timestamp features\n",
    "y_train = train['TARGET']\n",
    "X_test = test['value']  # Include the timestamp features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#confusion = confusion_matrix(y_test, y_pred)\n",
    "#classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Calculate precision and recall\n",
    "#precision = precision_score(y_test, y_pred)\n",
    "#recall = recall_score(y_test, y_pred)\n",
    "\n",
    "#print(f\"Precision: {precision}\")\n",
    "#print(f\"Recall: {recall}\")\n",
    "\n",
    "#print(f\"P&R Score: {(precision+recall)/2}\")\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "\n",
    "print(submission)\n",
    "\n",
    "submission['TARGET'] = y_pred\n",
    "submission = submission.reset_index(drop=True)\n",
    "\n",
    "submission.to_csv('submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a68a520a-6be1-4882-abbc-f795ab4acaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  TARGET\n",
      "0      50000       0\n",
      "1      50001       0\n",
      "2      50002       0\n",
      "3      50003       0\n",
      "4      50004       0\n",
      "...      ...     ...\n",
      "49995  99995       0\n",
      "49996  99996       0\n",
      "49997  99997       0\n",
      "49998  99998       0\n",
      "49999  99999       0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Assuming you have your data in a DataFrame 'train' with columns 'timestamp', 'value', and 'TARGET'\n",
    "\n",
    "# Convert the 'timestamp' column to datetime type\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "# Extract timestamp features\n",
    "train['hour'] = train['timestamp'].dt.hour\n",
    "train['dayofweek'] = train['timestamp'].dt.dayofweek\n",
    "train['month'] = train['timestamp'].dt.month\n",
    "\n",
    "test['hour'] = test['timestamp'].dt.hour\n",
    "test['dayofweek'] = test['timestamp'].dt.dayofweek\n",
    "test['month'] = test['timestamp'].dt.month\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X_train = train[['value', 'hour', 'dayofweek', 'month']]  # Include the timestamp features\n",
    "y_train = train['TARGET']\n",
    "X_test = test[['value', 'hour', 'dayofweek', 'month']]  # Include the timestamp features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a K-Nearest Neighbors model with a specified number of neighbors (e.g., n_neighbors=5)\n",
    "model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#confusion = confusion_matrix(y_test, y_pred)\n",
    "#classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Calculate precision and recall\n",
    "#precision = precision_score(y_test, y_pred)\n",
    "#recall = recall_score(y_test, y_pred)\n",
    "\n",
    "#print(f\"Precision: {precision}\")\n",
    "#print(f\"Recall: {recall}\")\n",
    "\n",
    "#print(f\"P&R Score: {(precision+recall)/2}\")\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "\n",
    "print(submission)\n",
    "\n",
    "submission['TARGET'] = y_pred\n",
    "submission = submission.reset_index(drop=True)\n",
    "\n",
    "submission.to_csv('submission_knn.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50015f86-ffef-42aa-b9ba-175714d9dc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/1pxkz68n6bl1f5638csbg2l00000gn/T/ipykernel_79721/107199016.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['LOF_scores'] = lof_scores  # Include LOF scores in the training data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime type\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "# Extract timestamp features\n",
    "train['hour'] = train['timestamp'].dt.hour\n",
    "train['dayofweek'] = train['timestamp'].dt.dayofweek\n",
    "train['month'] = train['timestamp'].dt.month\n",
    "\n",
    "test['hour'] = test['timestamp'].dt.hour\n",
    "test['dayofweek'] = test['timestamp'].dt.dayofweek\n",
    "test['month'] = test['timestamp'].dt.month\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X_train = train[['value', 'hour', 'dayofweek', 'month']]  # Include the timestamp features\n",
    "y_train = train['TARGET']\n",
    "X_test = test[['value', 'hour', 'dayofweek', 'month']]  # Include the timestamp features\n",
    "\n",
    "# Add Local Outlier Factor (LOF) scores as features\n",
    "lof_model = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "lof_scores = lof_model.fit_predict(X_train)\n",
    "X_train['LOF_scores'] = lof_scores  # Include LOF scores in the training data\n",
    "X_test_with_lof = X_test.copy()  # Create a copy of the test data\n",
    "lof_scores_test = lof_model.fit_predict(X_test)\n",
    "X_test_with_lof['LOF_scores'] = lof_scores_test  # Include LOF scores in the test data\n",
    "\n",
    "# Create a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth = 10, criterion= 'entropy', random_state=42)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Train the model on the training data with LOF scores\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data with LOF scores\n",
    "y_pred = model.predict(X_test_with_lof)\n",
    "\n",
    "# Optionally, you can evaluate the model's performance using various metrics such as accuracy.\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission['TARGET'] = y_pred\n",
    "submission.to_csv('submission_rf_lof.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b205e532-b8c0-47cc-813e-927ad0a49a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/1pxkz68n6bl1f5638csbg2l00000gn/T/ipykernel_79721/157474094.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['LOF_scores'] = lof_scores  # Include LOF scores in the training data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime type\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "# Extract timestamp features\n",
    "train['hour'] = train['timestamp'].dt.hour\n",
    "train['dayofweek'] = train['timestamp'].dt.dayofweek\n",
    "train['month'] = train['timestamp'].dt.month\n",
    "\n",
    "test['hour'] = test['timestamp'].dt.hour\n",
    "test['dayofweek'] = test['timestamp'].dt.dayofweek\n",
    "test['month'] = test['timestamp'].dt.month\n",
    "\n",
    "# Identify the missing dates between 6.02 and 6.04\n",
    "missing_dates = pd.date_range(start='2023-06-02', end='2023-06-04', freq='T')\n",
    "\n",
    "# Create an Exponential Smoothing model to forecast missing values\n",
    "forecast_model = ExponentialSmoothing(train['value'], seasonal='add', seasonal_periods=13)  # Adjust the seasonal parameters\n",
    "forecast_model = forecast_model.fit()\n",
    "\n",
    "# Fill in the missing values in the training data for the specified dates\n",
    "for date in missing_dates:\n",
    "    forecasted_value = forecast_model.forecast(1).iloc[0]\n",
    "    train.loc[train['timestamp'] == date, 'value'] = forecasted_value\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X_train = train[['value', 'hour', 'dayofweek', 'month']]\n",
    "y_train = train['TARGET']\n",
    "X_test = test[['value', 'hour', 'dayofweek', 'month']]\n",
    "\n",
    "# Add Local Outlier Factor (LOF) scores as features\n",
    "lof_model = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "lof_scores = lof_model.fit_predict(X_train)\n",
    "X_train['LOF_scores'] = lof_scores  # Include LOF scores in the training data\n",
    "X_test_with_lof = X_test.copy()  # Create a copy of the test data\n",
    "lof_scores_test = lof_model.fit_predict(X_test)\n",
    "X_test_with_lof['LOF_scores'] = lof_scores_test  # Include LOF scores in the test data\n",
    "\n",
    "# Create a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=200,  # Increase the number of trees\n",
    "    max_depth=10,     # Limit the depth of trees to avoid overfitting\n",
    "    min_samples_split=2,  # Adjust the minimum samples required to split a node\n",
    "    min_samples_leaf=1,   # Adjust the minimum samples in a leaf node\n",
    "    max_features='sqrt',  # Use square root of the number of features for splits\n",
    "    criterion='gini',     # Measure quality of split using Gini impurity\n",
    "    random_state=42,      # Set a random seed for reproducibility\n",
    "    class_weight='balanced',  # Assign weights to classes for imbalanced datasets\n",
    "    bootstrap=True,       # Use bootstrapping to sample data\n",
    "    oob_score=True        # Calculate OOB score)  # You can adjust the number of estimators as needed)\n",
    "                              )\n",
    "# Train the model on the training data with LOF scores\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data with LOF scores\n",
    "y_pred = model.predict(X_test_with_lof)\n",
    "\n",
    "# Optionally, you can evaluate the model's performance using various metrics such as accuracy.\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission['TARGET'] = y_pred\n",
    "submission.to_csv('submission_rf_lof_stl.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "908eaf0a-470c-43d7-bda5-1c2e8aa25f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/1pxkz68n6bl1f5638csbg2l00000gn/T/ipykernel_79721/3576310793.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['LOF_scores'] = lof_scores  # Include LOF scores in the training data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- is_anomaly\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data with LOF scores\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_with_lof\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Optionally, you can evaluate the model's performance using various metrics such as accuracy.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# accuracy = accuracy_score(y_test, y_pred)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Save the predictions to a CSV file\u001b[39;00m\n\u001b[1;32m     68\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTARGET\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_pred\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    517\u001b[0m ):\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    503\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- is_anomaly\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy import stats\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime type\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "# Extract timestamp features\n",
    "train['hour'] = train['timestamp'].dt.hour\n",
    "train['dayofweek'] = train['timestamp'].dt.dayofweek\n",
    "train['month'] = train['timestamp'].dt.month\n",
    "\n",
    "test['hour'] = test['timestamp'].dt.hour\n",
    "test['dayofweek'] = test['timestamp'].dt.dayofweek\n",
    "test['month'] = test['timestamp'].dt.month\n",
    "\n",
    "# Identify the missing dates between 6.02 and 6.04\n",
    "missing_dates = pd.date_range(start='2023-06-02', end='2023-06-04', freq='T')\n",
    "\n",
    "# Create an Exponential Smoothing model to forecast missing values\n",
    "forecast_model = ExponentialSmoothing(train['value'], seasonal='add', seasonal_periods=13)  # Adjust the seasonal parameters\n",
    "forecast_model = forecast_model.fit()\n",
    "\n",
    "# Fill in the missing values in the training data for the specified dates\n",
    "for date in missing_dates:\n",
    "    forecasted_value = forecast_model.forecast(1).iloc[0]\n",
    "    train.loc[train['timestamp'] == date, 'value'] = forecasted_value\n",
    "\n",
    "# Calculate Z-Scores and flag anomalies\n",
    "train['z_score'] = stats.zscore(train['value'])\n",
    "threshold = 2  # You can adjust the threshold as needed\n",
    "train['is_anomaly'] = train['z_score'].apply(lambda x: 1 if abs(x) > threshold else 0)\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X_train = train[['value', 'hour', 'dayofweek', 'month', 'is_anomaly']]\n",
    "y_train = train['TARGET']\n",
    "X_test = test[['value', 'hour', 'dayofweek', 'month']]\n",
    "\n",
    "# Add Local Outlier Factor (LOF) scores as features\n",
    "lof_model = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "lof_scores = lof_model.fit_predict(X_train)\n",
    "X_train['LOF_scores'] = lof_scores  # Include LOF scores in the training data\n",
    "X_test_with_lof = X_test.copy()  # Create a copy of the test data\n",
    "lof_scores_test = lof_model.fit_predict(X_test)\n",
    "X_test_with_lof['LOF_scores'] = lof_scores_test  # Include LOF scores in the test data\n",
    "\n",
    "# Create a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Train the model on the training data with LOF scores\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data with LOF scores\n",
    "y_pred = model.predict(X_test_with_lof)\n",
    "\n",
    "# Optionally, you can evaluate the model's performance using various metrics such as accuracy.\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission['TARGET'] = y_pred\n",
    "submission.to_csv('submission_rf_lof_zscore.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daabb19-3e61-4535-8f39-80fe5d6ac6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime type\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "# Extract timestamp features\n",
    "train['hour'] = train['timestamp'].dt.hour\n",
    "train['dayofweek'] = train['timestamp'].dt.dayofweek\n",
    "train['month'] = train['timestamp'].dt.month\n",
    "\n",
    "test['hour'] = test['timestamp'].dt.hour\n",
    "test['dayofweek'] = test['timestamp'].dt.dayofweek\n",
    "test['month'] = test['timestamp'].dt.month\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X_train = train[['value', 'hour', 'dayofweek', 'month']]  # Include the timestamp features\n",
    "y_train = train['TARGET']\n",
    "X_test = test[['value', 'hour', 'dayofweek', 'month']]  # Include the timestamp features\n",
    "\n",
    "# Create a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# confusion = confusion_matrix(y_test, y_pred)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Optionally, you can calculate precision and recall\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "\n",
    "# print(f\"P&R Score: {(precision+recall)/2}\")\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "\n",
    "print(submission)\n",
    "\n",
    "submission['TARGET'] = y_pred\n",
    "submission = submission.reset_index(drop=True)\n",
    "\n",
    "submission.to_csv('submission_rf1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "556d88b4-5374-4388-a112-643e804fc8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  TARGET\n",
      "0      50000       0\n",
      "1      50001       0\n",
      "2      50002       0\n",
      "3      50003       0\n",
      "4      50004       0\n",
      "...      ...     ...\n",
      "49995  99995       0\n",
      "49996  99996       0\n",
      "49997  99997       0\n",
      "49998  99998       0\n",
      "49999  99999       0\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime type\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "# Prepare the feature matrix (X) and the target variable (y)\n",
    "X_train = train[['value']]  # Use 'value' as the feature\n",
    "y_train = train['TARGET']\n",
    "X_test = test[['value']]  # Use 'value' as the feature\n",
    "\n",
    "# Create a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Optionally, you can evaluate the model's performance using various metrics such as accuracy.\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# confusion = confusion_matrix(y_test, y_pred)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Calculate precision and recall\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "\n",
    "# print(f\"P&R Score: {(precision+recall)/2}\")\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "\n",
    "print(submission)\n",
    "\n",
    "submission['TARGET'] = y_pred\n",
    "submission = submission.reset_index(drop=True)\n",
    "\n",
    "submission.to_csv('submission_rf2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045048e-9bfd-4e89-8360-fb3397f90f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
